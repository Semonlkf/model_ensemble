"""
FastAPI æœåŠ¡: math-shepherd-mistral-7b-prm å¥–åŠ±æ¨¡å‹

æ¨¡å‹æ¥æº: peiyi9979/math-shepherd-mistral-7b-prm
æ¨¡å‹ç‰¹ç‚¹: 
    - ä½¿ç”¨ç‰¹æ®Š token 'ĞºĞ¸' æ ‡è®°æ¯ä¸ªæ¨ç†æ­¥éª¤çš„ç»“æŸ
    - è¾“å‡º '+' (æ­£ç¡®) å’Œ '-' (é”™è¯¯) çš„æ¦‚ç‡ä½œä¸ºæ­¥éª¤åˆ†æ•°

å¯åŠ¨æ–¹å¼:
    # æ–¹å¼1: ä½¿ç”¨ --gpu å‚æ•°æŒ‡å®š GPU
    python serves/math_shepherd_prm.py --port 8001 --gpu 0
    
    # æ–¹å¼2: ä½¿ç”¨ CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡
    CUDA_VISIBLE_DEVICES=1 python serves/math_shepherd_prm.py --port 8001
    
    # æ–¹å¼3: æŒ‡å®šæœ¬åœ°æ¨¡å‹è·¯å¾„
    python serves/math_shepherd_prm.py --model_path /path/to/local/model --port 8001

API æ¥å£è§„èŒƒ (æ‰€æœ‰å¥–åŠ±æ¨¡å‹æœåŠ¡å¿…é¡»éµå¾ª):
    POST /v1/scores
    è¯·æ±‚ä½“: {"model": "model_name", "input": "text with step markers"}
    å“åº”ä½“: {"data": [{"score": float, "step_scores": [{"step_index": int, "score": float}, ...]}]}
"""

import os
import argparse
from typing import List, Optional
from contextlib import asynccontextmanager

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import AutoTokenizer, AutoModelForCausalLM


# ============ å…¨å±€å˜é‡ ============
model = None
tokenizer = None
candidate_tokens = None
step_tag_id = None
device = None

# ============ Math-Shepherd ç‰¹æ®Š token å®šä¹‰ ============
GOOD_TOKEN = '+'
BAD_TOKEN = '-'
STEP_TAG = 'ĞºĞ¸'  # Math-Shepherd ä½¿ç”¨æ­¤ token æ ‡è®°æ­¥éª¤ç»“æŸ


# ============ é€šç”¨è¯·æ±‚/å“åº”æ¨¡å‹ (æ‰€æœ‰å¥–åŠ±æ¨¡å‹æœåŠ¡å…±ç”¨) ============
class ScoreRequest(BaseModel):
    """
    å¥–åŠ±æ¨¡å‹è¯„åˆ†è¯·æ±‚ (é€šç”¨æ¥å£)
    
    Attributes:
        model: æ¨¡å‹åç§°æ ‡è¯†ç¬¦
        input: å¾…è¯„åˆ†çš„æ–‡æœ¬ï¼ŒåŒ…å«é—®é¢˜å’Œæ¨ç†æ­¥éª¤
    """
    model: str
    input: str


class ScoreResponse(BaseModel):
    """
    å¥–åŠ±æ¨¡å‹è¯„åˆ†å“åº” (é€šç”¨æ¥å£)
    
    Attributes:
        data: è¯„åˆ†ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«:
            - score: æ•´ä½“è¯„åˆ† (float)
            - step_scores: å„æ­¥éª¤è¯„åˆ†åˆ—è¡¨ [{"step_index": int, "score": float}, ...]
    """
    data: List[dict]


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    model_loaded: bool
    model_name: Optional[str] = None


# ============ Math-Shepherd æ¨¡å‹åŠ è½½ ============
def load_model(model_path: str, gpu_id: Optional[int] = None):
    """
    åŠ è½½ Math-Shepherd å¥–åŠ±æ¨¡å‹
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„ (HuggingFace æˆ–æœ¬åœ°è·¯å¾„)
        gpu_id: æŒ‡å®š GPU ID (0, 1, 2, ...)ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ CUDA_VISIBLE_DEVICES æˆ–é»˜è®¤ GPU
    """
    global model, tokenizer, candidate_tokens, step_tag_id, device
    
    # ç¡®å®šä½¿ç”¨çš„è®¾å¤‡
    if not torch.cuda.is_available():
        device = torch.device("cpu")
        print("âš ï¸ CUDA not available, using CPU")
    elif gpu_id is not None:
        device = torch.device(f"cuda:{gpu_id}")
        print(f"ğŸ¯ Using GPU {gpu_id}")
    else:
        device = torch.device("cuda")
        print(f"ğŸ¯ Using default GPU (CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')})")
    
    print(f"ğŸ”„ Loading Math-Shepherd tokenizer from {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # è·å– Math-Shepherd ç‰¹æ®Š token çš„ ID
    candidate_tokens = tokenizer.encode(f"{GOOD_TOKEN} {BAD_TOKEN}")[1:]  # [648, 387]
    step_tag_id = tokenizer.encode(f"{STEP_TAG}")[-1]  # 12902
    
    print(f"ğŸ“ Candidate tokens ('+', '-'): {candidate_tokens}")
    print(f"ğŸ“ Step tag ID ('ĞºĞ¸'): {step_tag_id}")
    
    print(f"ğŸ”„ Loading Math-Shepherd model from {model_path}...")
    
    # æ ¹æ®è®¾å¤‡ç±»å‹é€‰æ‹©åŠ è½½æ–¹å¼
    if device.type == "cuda":
        if gpu_id is not None:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map={"": device}
            ).eval()
        else:
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16,
                device_map="auto"
            ).eval()
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32
        ).eval().to(device)
    
    print(f"âœ… Math-Shepherd model loaded on {device}")


# ============ Math-Shepherd æ¨ç†é€»è¾‘ ============
def compute_step_scores(text: str) -> dict:
    """
    è®¡ç®— Math-Shepherd æ¯ä¸ªæ¨ç†æ­¥éª¤çš„å¾—åˆ†
    
    Math-Shepherd æ¨¡å‹è¦æ±‚:
        - æ¯ä¸ªæ­¥éª¤ä»¥ 'ĞºĞ¸' token ç»“å°¾
        - æ¨¡å‹åœ¨ 'ĞºĞ¸' ä½ç½®è¾“å‡º '+' å’Œ '-' çš„æ¦‚ç‡
        - '+' çš„æ¦‚ç‡è¶Šé«˜è¡¨ç¤ºè¯¥æ­¥éª¤è¶Šæ­£ç¡®
    
    Args:
        text: åŒ…å«é—®é¢˜å’Œæ¨ç†æ­¥éª¤çš„æ–‡æœ¬ï¼Œæ ¼å¼å¦‚:
              "Question... Step 1: xxx ĞºĞ¸ Step 2: xxx ĞºĞ¸ ..."
        
    Returns:
        dict: {
            "score": float,  # æ•´ä½“è¯„åˆ† (å–æ‰€æœ‰æ­¥éª¤çš„æœ€å°åˆ†æ•°)
            "step_scores": [{"step_index": int, "score": float}, ...]
        }
    """
    global model, tokenizer, candidate_tokens, step_tag_id, device
    
    input_ids = torch.tensor([tokenizer.encode(text)]).to(device)
    
    with torch.no_grad():
        logits = model(input_ids).logits[:, :, candidate_tokens]
        # softmax åå–ç¬¬ä¸€ä¸ª token (good token '+') çš„æ¦‚ç‡ä½œä¸ºåˆ†æ•°
        scores = logits.softmax(dim=-1)[:, :, 0]
        
        # æ‰¾åˆ°æ‰€æœ‰ step_tag ('ĞºĞ¸') ä½ç½®çš„åˆ†æ•°
        mask = input_ids[0] == step_tag_id
        step_scores = scores[0][mask].cpu().tolist()
    
    # è®¡ç®—æ•´ä½“åˆ†æ•°
    if step_scores:
        # ä½¿ç”¨æœ€å°åˆ†æ•°ä½œä¸ºæ•´ä½“è¯„åˆ† (æœ€å¼±æ­¥éª¤å†³å®šæ•´ä½“è´¨é‡)
        overall_score = min(step_scores)
    else:
        overall_score = 0.0
    
    return {
        "score": overall_score,
        "step_scores": [
            {"step_index": i, "score": s} 
            for i, s in enumerate(step_scores)
        ]
    }


# ============ FastAPI åº”ç”¨ ============
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    yield
    print("ğŸ›‘ Shutting down Math-Shepherd server...")


app = FastAPI(
    title="Math-Shepherd PRM API",
    description="FastAPI æœåŠ¡: math-shepherd-mistral-7b-prm è¿‡ç¨‹å¥–åŠ±æ¨¡å‹",
    version="1.0.0",
    lifespan=lifespan
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return HealthResponse(
        status="ok" if model is not None else "not_ready",
        model_loaded=model is not None,
        model_name="math-shepherd-mistral-7b-prm" if model else None
    )


@app.post("/v1/scores", response_model=ScoreResponse)
async def get_scores(request: ScoreRequest):
    """
    è·å–å¥–åŠ±åˆ†æ•°
    
    è¯·æ±‚ä½“ç¤ºä¾‹:
        {
            "model": "math-shepherd-mistral-7b-prm",
            "input": "Janet's ducks lay 16 eggs per day... Step 1: xxx ĞºĞ¸ Step 2: xxx ĞºĞ¸"
        }
    
    å“åº”ä½“ç¤ºä¾‹:
        {
            "data": [{
                "score": 0.95,
                "step_scores": [
                    {"step_index": 0, "score": 0.99},
                    {"step_index": 1, "score": 0.95}
                ]
            }]
        }
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        result = compute_step_scores(request.input)
        return ScoreResponse(data=[result])
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# å…¼å®¹ä¸å¸¦ /v1 å‰ç¼€çš„è·¯å¾„
@app.post("/scores", response_model=ScoreResponse)
async def get_scores_compat(request: ScoreRequest):
    """å…¼å®¹æ—§ç‰ˆ API è·¯å¾„"""
    return await get_scores(request)


def main():
    import uvicorn
    
    parser = argparse.ArgumentParser(description="Math-Shepherd PRM FastAPI Server")
    parser.add_argument("--model_path", type=str, default="/mnt/shared-storage-gpfs2/gpfs2-shared-public/huggingface/zskj-hub/models--peiyi9979--math-shepherd-mistral-7b-prm",
                        help="HuggingFace model path or local path")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="Host to bind")
    parser.add_argument("--port", type=int, default=8001, help="Port to bind")
    parser.add_argument("--gpu", type=int, default=None, 
                        help="GPU ID to use (e.g., 0, 1, 2). If not specified, uses CUDA_VISIBLE_DEVICES or default GPU")
    
    args = parser.parse_args()
    
    load_model(args.model_path, gpu_id=args.gpu)
    
    print(f"ğŸš€ Starting Math-Shepherd PRM Server at http://{args.host}:{args.port}")
    uvicorn.run(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main()

