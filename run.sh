#!/bin/bash
# ============================================================
# æ¨¡å‹æœåŠ¡å¯åŠ¨è„šæœ¬
# 
# éƒ¨ç½²è§„åˆ™:
#   - ç”Ÿæˆæ¨¡å‹: ä½¿ç”¨ vLLM éƒ¨ç½²
#   - å¥–åŠ±æ¨¡å‹: ä½¿ç”¨ FastAPI éƒ¨ç½² (serves/math_shepherd_prm.py)
# ============================================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# ============ é…ç½®å‚æ•° ============
DATE=$(date +"%Y%m%d")
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
LOG_DIR="serve_logs/${DATE}"

# ç«¯å£é…ç½®
GENERATION_MODEL_PORT=23456
REWARD_MODEL_PORT=8001

# GPU é…ç½® (æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
GENERATION_GPU=0
REWARD_GPU=1

# æ¨¡å‹è·¯å¾„ (æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
GENERATION_MODEL_PATH="/mnt/shared-storage-user/marti/models/Qwen3-8B"
REWARD_MODEL_PATH="peiyi9979/math-shepherd-mistral-7b-prm"

# ============ åˆ›å»ºæ—¥å¿—ç›®å½• ============
mkdir -p "$LOG_DIR"
echo "ğŸ“ Log directory: $LOG_DIR"

# ============ å¯åŠ¨å¥–åŠ±æ¨¡å‹ (FastAPI) ============
echo "ğŸš€ Starting Reward Model (Math-Shepherd) on GPU $REWARD_GPU, port $REWARD_MODEL_PORT..."
nohup python serves/math_shepherd_prm.py \
    --model_path "$REWARD_MODEL_PATH" \
    --port $REWARD_MODEL_PORT \
    --gpu $REWARD_GPU \
    > "$LOG_DIR/math_shepherd_prm_${TIMESTAMP}.log" 2>&1 &
REWARD_PID=$!
echo "   PID: $REWARD_PID"

# ============ å¯åŠ¨ç”Ÿæˆæ¨¡å‹ (vLLM) ============
echo "ğŸš€ Starting Generation Model (Qwen3-8B) on GPU $GENERATION_GPU, port $GENERATION_MODEL_PORT..."
nohup env CUDA_VISIBLE_DEVICES=$GENERATION_GPU vllm serve "$GENERATION_MODEL_PATH" \
    --port $GENERATION_MODEL_PORT \
    --max-num-seqs 512 \
    --gpu-memory-utilization 0.8 \
    --max-logprobs 10 \
    > "$LOG_DIR/vllm_qwen3_8b_${TIMESTAMP}.log" 2>&1 &
VLLM_PID=$!
echo "   PID: $VLLM_PID"

# ============ ä¿å­˜ PID ä¿¡æ¯ ============
echo "REWARD_PID=$REWARD_PID" > "$LOG_DIR/pids_${TIMESTAMP}.txt"
echo "VLLM_PID=$VLLM_PID" >> "$LOG_DIR/pids_${TIMESTAMP}.txt"
echo "ğŸ“ PIDs saved to $LOG_DIR/pids_${TIMESTAMP}.txt"

# ============ ç­‰å¾…æœåŠ¡å¯åŠ¨ ============
echo ""
echo "â³ Waiting for services to start..."
echo "   - Reward Model log: $LOG_DIR/math_shepherd_prm_${TIMESTAMP}.log"
echo "   - Generation Model log: $LOG_DIR/vllm_qwen3_8b_${TIMESTAMP}.log"
echo ""

# ç­‰å¾… vLLM å¯åŠ¨ (é€šå¸¸éœ€è¦ 30-60 ç§’)
sleep 10

# ============ å¥åº·æ£€æŸ¥ ============
echo "ğŸ” Checking service health..."

# æ£€æŸ¥å¥–åŠ±æ¨¡å‹
if curl -s "http://localhost:$REWARD_MODEL_PORT/health" > /dev/null 2>&1; then
    echo "   âœ… Reward Model is ready"
else
    echo "   â³ Reward Model is still loading (check log for details)"
fi

# æ£€æŸ¥ç”Ÿæˆæ¨¡å‹
if curl -s "http://localhost:$GENERATION_MODEL_PORT/health" > /dev/null 2>&1; then
    echo "   âœ… Generation Model is ready"
else
    echo "   â³ Generation Model is still loading (check log for details)"
fi

echo ""
echo "============================================================"
echo "ğŸ“‹ Service Summary:"
echo "   Reward Model:     http://localhost:$REWARD_MODEL_PORT (GPU $REWARD_GPU)"
echo "   Generation Model: http://localhost:$GENERATION_MODEL_PORT (GPU $GENERATION_GPU)"
echo ""
echo "ğŸ“‚ Logs: $LOG_DIR/"
echo ""
echo "ğŸ›‘ To stop services:"
echo "   kill $REWARD_PID $VLLM_PID"
echo "   or: kill \$(cat $LOG_DIR/pids_${TIMESTAMP}.txt | cut -d'=' -f2 | tr '\\n' ' ')"
echo "============================================================"

# ============ å¯é€‰: è¿è¡Œä¸»ç¨‹åº ============
# å–æ¶ˆä¸‹é¢çš„æ³¨é‡Šä»¥è‡ªåŠ¨è¿è¡Œä¸»ç¨‹åº
# echo ""
# echo "ğŸƒ Starting main program..."
python -u run.py \
    --backend Qwen3-8B \
    --task gsm8k \
    --task_start_index 0 \
    --task_end_index 100 \
    --prompt_sample cot \
    --method_generate sample \
    --method_evaluate random \
    --n_generate_sample 4 \
    --n_evaluate_sample 3 \
    --baseline mcts \
    --model_pool_config configs/ensemble_example.yaml

python -u run.py \
    --task gsm8k \
    --task_start_index 0 \
    --task_end_index 100 \
    --prompt_sample cot \
    --method_generate sample \
    --method_evaluate llm_as_process_reward \
    --baseline lemcts \
    --backend_prm math_shepherd \
    --model_pool_config configs/ensemble_example.yaml